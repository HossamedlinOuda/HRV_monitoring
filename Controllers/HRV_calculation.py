# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BoNJABMbV7g4nM6OfXW0ul7aWnZeppHW
"""
import sys
# import heartpy as hp
# import matplotlib.pyplot as plt
# print("Inside the python file")
# arr=sys.argv[1]
# print("HELLLLLLLLLLLLLOOO" , arr)


"""Let's look at the first file and visualise it:"""

# filename = 'D:/Graduation project/Sensor readings/Hossamfiltered.csv'


# data  = arr
# print("Befroe new data assign")
# newdata = hp.enhance_peaks(data,iterations=4)
# plt.figure(figsize=(12,4))
# plt.plot(newdata)
# plt.show()

# and zoom in a bit
# plt.figure(figsize=(12,4))
# plt.plot(newdata[0:250])
# plt.show()
# print(newdata)
# print(type(newdata))

# print("before the process phase")
# wd, m = hp.process(hp.scale_data(data), 11.1, calc_freq = True,high_precision=True, reject_segmentwise = True, clean_rr = True)

# visualise in plot of custom size
# plt.figure(figsize=(12,4))
# hp.plotter(wd, m)

# display computed measures
# print("---------------->>>" , m)
# for measure in m.keys():
#     print('%s: %f' %(measure, m[measure]))

import subprocess
subprocess.check_call([sys.executable, "-m", "pip", "install", 'pandas'])
subprocess.check_call([sys.executable, "-m", "pip", "install", 'scipy'])

import os
import sys
import math
import pandas as pd
import numpy as np
from scipy import signal
from scipy.ndimage import label
from scipy.stats import zscore
from scipy.interpolate import interp1d
from scipy.integrate import trapz


def detect_peaks(ecg_signal, threshold=0.3, qrs_filter=None):
    '''
    Peak detection algorithm using cross corrrelation and threshold 
    '''
    if qrs_filter is None:
        # create default qrs filter, which is just a part of the sine function
        t = np.linspace(1.5 * np.pi, 3.5 * np.pi, 15)
        qrs_filter = np.sin(t)
    
    # normalize data
    ecg_signal = (ecg_signal - ecg_signal.mean()) / ecg_signal.std()

    # calculate cross correlation
    similarity = np.correlate(ecg_signal, qrs_filter, mode="same")
    similarity = similarity / np.max(similarity)

    # return peaks (values in ms) using threshold
    return ecg_signal[similarity > threshold].index, similarity

def get_plot_ranges(start=10, end=20, n=5):
    distance = end - start
    for i in np.arange(start, end, np.floor(distance/n)):
        yield (int(i), int(np.minimum(end, np.floor(distance/n) + i)))

def group_peaks(p, threshold=5):
    '''
    The peak detection algorithm finds multiple peaks for each QRS complex. 
    Here we group collections of peaks that are very near (within threshold) and we take the median index 
    '''
    # initialize output
    output = np.empty(0)

    # label groups of sample that belong to the same peak
    peak_groups, num_groups = label(np.diff(p) < threshold)

    # iterate through groups and take the mean as peak index
    for i in np.unique(peak_groups)[1:]:
        peak_group = p[np.where(peak_groups == i)]
        output = np.append(output, np.median(peak_group))
    return output

def timedomain(rr):
    results = {}

    hr = 60000/rr

    nn_intervals = [x - y for x, y in zip(rr[1:], rr[:-1])]
    rr_pairs = np.array([nn_intervals[:-1], nn_intervals[1:]])
    cov = np.cov(rr_pairs)
    eigvals, eigvecs = np.linalg.eig(cov)
    sd1 = np.sqrt(eigvals.min())
    sd2 = np.sqrt(eigvals.max())
    mean_nn = sum(nn_intervals) / len(nn_intervals)
    nn_diff = [abs(nn_intervals[i+1] - nn_intervals[i]) for i in range(len(nn_intervals)-1)]

    
    results['Mean RR (ms)'] = np.mean(rr)
    results['SDNN '] = math.sqrt(sum([(x - mean_nn)**2 for x in nn_intervals]) / (len(nn_intervals) - 1))
    results['sd1sd2 (ms)'] = sd1/sd2
    results['STD RR/SDNN (ms)'] = np.std(rr)
    results['Mean HR (Kubios\' style) (beats/min)'] = 60000/np.mean(rr)
    results['Mean HR (beats/min)'] = np.mean(hr)
    results['STD HR (beats/min)'] = np.std(hr)
    results['Min HR (beats/min)'] = np.min(hr)
    results['Max HR (beats/min)'] = np.max(hr)
    results['RMSSD (ms)'] = np.sqrt(np.mean(np.square(np.diff(rr))))
    results['NNxx'] = np.sum(np.abs(np.diff(rr)) > 50)*1
    results['pNNxx (%)'] = 100 * np.sum((np.abs(np.diff(rr)) > 50)*1) / len(rr)
    results['pNN50'] =(sum([1 for diff in nn_diff if diff > 50]) / len(nn_diff)) * 100
    return results

df = pd.read_csv("Controllers/ecg.csv", sep=";", index_col="ms")

sampfrom = 60000
sampto = 70000
nr_plots = 1
 



peaks, similarity = detect_peaks(df.heartrate, threshold=0.3)

# group peaks
grouped_peaks = group_peaks(peaks)
peaks, similarity = detect_peaks(df.heartrate, threshold=0.3)

# group peaks so we get a single peak per beat (hopefully)
grouped_peaks = group_peaks(peaks)

# RR-intervals are the differences between successive peaks
rr = np.diff(grouped_peaks)

outlier_low = np.mean(rr) - 2 * np.std(rr)
outlier_high = np.mean(rr) + 2 * np.std(rr)

rr_corrected = rr.copy()
rr_corrected[np.abs(zscore(rr)) > 2] = np.median(rr)

sampfrom = 240000
sampto = 250000
nr_plots = 1

# detect peaks
peaks, similarity = detect_peaks(df.heartrate, threshold=0.3)

# group peaks so we get a single peak per beat (hopefully)
grouped_peaks = group_peaks(peaks)

# RR-intervals are the differences between successive peaks
rr = np.diff(grouped_peaks)

print("Time domain metrics - automatically corrected RR-intervals:")
for k, v in timedomain(rr).items():
    print("- %s: %.2f" % (k, v))

print()
print("Time domain metrics - manually corrected RR-intervals:")
for k, v in timedomain(rr_corrected).items():
    print("- %s: %.2f" % (k, v))